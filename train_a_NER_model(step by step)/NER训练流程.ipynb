{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = r'rmrb199801.txt' # 选择语料库正确的存储路径\n",
    "text = open(data_path, encoding='utf-8').read()\n",
    "text =  re.sub('199801.{13}/m  ', '', text) # 删除开头的时间戳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义原始数据预处理函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     17,
     41,
     65
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def q_to_b(q_str):\n",
    "    \"\"\"\n",
    "    功能：非中文文字的全角转半角\n",
    "    输入：一个字符串\n",
    "    输出：半角字符串\n",
    "    \n",
    "    \"\"\"\n",
    "    b_str = \"\"\n",
    "    for uchar in q_str:\n",
    "        inside_code = ord(uchar)\n",
    "        if inside_code == 12288:  \n",
    "            inside_code = 32\n",
    "        elif 65374 >= inside_code >= 65281:  \n",
    "            inside_code -= 65248\n",
    "        b_str += chr(inside_code)\n",
    "    return b_str\n",
    "\n",
    "def process_t(words):\n",
    "    \"\"\"\n",
    "    功能：处理时间词，带有连续的时间词性的多个词进行合并\n",
    "    输入：由带有词性标签的字符组成的列表\n",
    "    输出：合并相邻并且带有'/t'词性标签的字符后的列表\n",
    "    \"\"\"\n",
    "    pro_words = []\n",
    "    index = 0\n",
    "    temp = u''\n",
    "    while True:\n",
    "        word = words[index] if index < len(words) else u''\n",
    "        if u'/t' in word:\n",
    "            temp = temp.replace(u'/t', u'') + word\n",
    "        elif temp:\n",
    "            pro_words.append(temp)\n",
    "            pro_words.append(word)\n",
    "            temp = u''\n",
    "        elif word:\n",
    "            pro_words.append(word)\n",
    "        else:\n",
    "            break\n",
    "        index += 1\n",
    "    return pro_words\n",
    "\n",
    "def process_nr(words):\n",
    "    \"\"\"\n",
    "    功能：处理姓名词，带有连续的姓名词性的多个词进行合并\n",
    "    输入：由带有词性标签的字符组成的列表\n",
    "    输出：合并相邻并且带有'/nr'词性标签的字符后的列表\n",
    "    \"\"\"\n",
    "    pro_words = []\n",
    "    index = 0\n",
    "    while True:\n",
    "        word = words[index] if index < len(words) else u''\n",
    "        if u'/nr' in word:\n",
    "            next_index = index + 1\n",
    "            if next_index < len(words) and u'/nr' in words[next_index]:\n",
    "                pro_words.append(word.replace(u'/nr', u'') + words[next_index])\n",
    "                index = next_index\n",
    "            else:\n",
    "                pro_words.append(word)\n",
    "        elif word:\n",
    "            pro_words.append(word)\n",
    "        else:\n",
    "            break\n",
    "        index += 1\n",
    "    return pro_words \n",
    "\n",
    "def process_k(words):\n",
    "    \"\"\"\n",
    "    功能：处理大粒度分词，包含在'[]'内的词可以作为组合词，将其合并，去掉两侧的'[]'\n",
    "    输入：由带有词性标签的字符组成的列表\n",
    "    输出：合并后'的列表\n",
    "    \"\"\"\n",
    "    pro_words = []\n",
    "    index = 0\n",
    "    temp = u''\n",
    "    while True:\n",
    "        word = words[index] if index < len(words) else u''\n",
    "        if u'[' in word:\n",
    "            temp += re.sub(pattern=u'/[a-zA-Z]*', repl=u'', string=word.replace(u'[', u''))\n",
    "        elif u']' in word:\n",
    "            w = word.split(u']')\n",
    "            temp += re.sub(pattern=u'/[a-zA-Z]*', repl=u'', string=w[0])\n",
    "            pro_words.append(temp+u'/'+w[1])\n",
    "            temp = u''\n",
    "        elif temp:\n",
    "            temp += re.sub(pattern=u'/[a-zA-Z]*', repl=u'', string=word)\n",
    "        elif word:\n",
    "            pro_words.append(word)\n",
    "        else:\n",
    "            break\n",
    "        index += 1\n",
    "    return pro_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理原始数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = q_to_b(text)\n",
    "text = text.split()\n",
    "text = process_k(text)\n",
    "text = process_nr(text)\n",
    "text = process_t(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 提取数据特征部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本部分函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     1,
     6,
     15,
     29,
     40,
     67,
     85
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#_maps提供一个由词性到实体标签的映射\n",
    "_maps = {u't': u'T',\n",
    "         u'nr': u'PER',\n",
    "         u'ns': u'LOC',\n",
    "         u'nt': u'ORG'}\n",
    "\n",
    "def pos_to_tag(p):\n",
    "    \"\"\"\n",
    "    功能：词性提取实体标签\n",
    "    输入：语料库中的词性字符，如't','nr'等\n",
    "    输出：对应的实体标签\n",
    "    \"\"\"\n",
    "    t = _maps.get(p, None)\n",
    "    return t if t else u'O'\n",
    "\n",
    "def tag_perform(tag, index):\n",
    "    \"\"\"\n",
    "    功能：根据实体中不同单字的位置为标签加上'B'或'I',使用BIO模式\n",
    "    输入：tag:实体标签\n",
    "          index:该标签在对应词语的标签组中的位置\n",
    "    输出：完整的分词标签\n",
    "    \"\"\"\n",
    "    if index == 0 and tag != u'O':\n",
    "        return u'B_{}'.format(tag)\n",
    "    elif tag != u'O':\n",
    "        return u'I_{}'.format(tag)\n",
    "    else:\n",
    "        return tag\n",
    "\n",
    "def pos_perform(pos):\n",
    "    \"\"\"\n",
    "    功能：对于同属名词的词性（'nr','ns','nt'），去除词性携带的标签先验知识\n",
    "    输入：词性（'nr','ns','nt'等）\n",
    "    输出：除去'n'的名词词性\n",
    "    \"\"\"\n",
    "    if pos in _maps.keys() and pos != u't':\n",
    "        return u'n'\n",
    "    else:\n",
    "        return pos\n",
    "\n",
    "def init_sequence(words_list):\n",
    "    \"\"\"\n",
    "    功能：初始化字序列、词性序列、实体标签序列 \n",
    "    输入：嵌套的列表，例如：[['迈向/v'], ['充满/v'], ['希望/n'], ['的/u'], ['新/a']]\n",
    "    输出：word_seq:去掉词性标签的词语列表的嵌套，例如：[['迈', '向'], \n",
    "                                                        ['充', '满'], \n",
    "                                                        ['希', '望'], \n",
    "                                                        ['的'], \n",
    "                                                        ['新']]\n",
    "          tag_seq:对应word_tag中每个元素的实体标识列表的嵌套，例如：[['O', 'O'], \n",
    "                                                                     ['O', 'O'], \n",
    "                                                                     ['O', 'O'], \n",
    "                                                                     ['O'], \n",
    "                                                                     ['O']]\n",
    "    \"\"\"\n",
    "    words_seq = [[word.split(u'/')[0] for word in words] for words in words_list]\n",
    "    pos_seq = [[word.split(u'/')[1] for word in words] for words in words_list]\n",
    "    tag_seq = [[pos_to_tag(p) for p in pos] for pos in pos_seq]\n",
    "    pos_seq = [[[pos_seq[index][i] for _ in range(len(words_seq[index][i]))]\n",
    "                for i in range(len(pos_seq[index]))] for index in range(len(pos_seq))]\n",
    "    tag_seq = [[[tag_perform(tag_seq[index][i], w) for w in range(len(words_seq[index][i]))]\n",
    "                for i in range(len(tag_seq[index]))] for index in range(len(tag_seq))]\n",
    "    pos_seq = [[u'un']+[pos_perform(p) for pos in pos_seq for p in pos]+[u'un'] for pos_seq in pos_seq]\n",
    "    tag_seq = [[t for tag in tag_seq for t in tag] for tag_seq in tag_seq]\n",
    "    word_seq = [[w for word in word_seq for w in word] for word_seq in words_seq]\n",
    "    return word_seq, tag_seq\n",
    "\n",
    "def segment_by_window(words_list=None, window=3):\n",
    "    \"\"\"\n",
    "    功能：对words_list中的每个元素，按window长度从第一个开始以步长为1进行窗口切分\n",
    "    输入：words_list:列表，例如：['<BOS>', '迈', '向', '充', '满']\n",
    "          window:切分长度，默认为3\n",
    "    输出：二层嵌套的列表，例如：[['<BOS>', '迈', '向'], \n",
    "                                 ['迈', '向', '充'], \n",
    "                                 ['向', '充', '满']]\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    begin, end = 0, window\n",
    "    for _ in range(1, len(words_list)):\n",
    "        if end > len(words_list): break\n",
    "        words.append(words_list[begin:end])\n",
    "        begin += 1\n",
    "        end += 1\n",
    "    return words\n",
    "\n",
    "def extract_feature(word_grams):\n",
    "    \"\"\"\n",
    "    功能：特征模板，即对每个单字抽取其此模板定义的特征\n",
    "    输入：三层列表的嵌套形式，并且加入词语的分隔标识符，例如：[[['<BOS>', '迈', '向'],\n",
    "                                                                ['迈', '向', '充'],\n",
    "                                                                ['向', '充', '满'],\n",
    "                                                                ['充', '满', '希'],\n",
    "                                                                ['满', '希', '望']]]\n",
    "    输出：list of list of dicts，例如：[[{'bias': 1.0, 'w': '迈', 'w+1': '向', 'w-1': '<BOS>',\n",
    "                                          'w-1:w': '<BOS>迈', 'w:w+1': '迈向'},\n",
    "                                         {'bias': 1.0, 'w': '向', 'w+1': '充', 'w-1': '迈',\n",
    "                                          'w-1:w': '迈向', 'w:w+1': '向充'},\n",
    "                                         {'bias': 1.0, 'w': '充', 'w+1': '满', 'w-1': '向',\n",
    "                                          'w-1:w': '向充', 'w:w+1': '充满'}, \n",
    "                                         {'bias': 1.0, 'w': '满', 'w+1': '希', 'w-1': '充',\n",
    "                                          'w-1:w': '充满', 'w:w+1': '满希'},\n",
    "                                         {'bias': 1.0, 'w': '希', 'w+1': '望', 'w-1': '满',\n",
    "                                          'w-1:w': '满希', 'w:w+1': '希望'}]]\n",
    "    \"\"\"\n",
    "    features, feature_list = [], []\n",
    "    for index in range(len(word_grams)):\n",
    "        for i in range(len(word_grams[index])):\n",
    "            word_gram = word_grams[index][i]\n",
    "            feature = {u'w-1': word_gram[0], u'w': word_gram[1], u'w+1': word_gram[2],\n",
    "                       u'w-1:w': word_gram[0]+word_gram[1], u'w:w+1': word_gram[1]+word_gram[2],\n",
    "                        # u'p-1': cls.pos_seq[index][i], u'p': cls.pos_seq[index][i+1],\n",
    "                        # u'p+1': cls.pos_seq[index][i+2],\n",
    "                        # u'p-1:p': cls.pos_seq[index][i]+cls.pos_seq[index][i+1],\n",
    "                        # u'p:p+1': cls.pos_seq[index][i+1]+cls.pos_seq[index][i+2],\n",
    "                        u'bias': 1.0}\n",
    "            feature_list.append(feature)\n",
    "        features.append(feature_list)\n",
    "        feature_list = []\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_list = [line.split() for line in text if line.strip()]\n",
    "# words_list's content likes '[['迈向/v'], ['充满/v'], ['希望/n'], ['的/u'], ['新/a'], ...]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_seq, tag_seq = init_sequence(words_list)\n",
    "# word_seq's content likes '[['迈', '向'], ['充', '满'], ['希', '望'], ['的'], ['新'], ...]'\n",
    "# tag_seq's content likes '[['O', 'O'], ['O', 'O'], ['O', 'O'], ['O'], ['O'], ...]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_list_word = []\n",
    "one_list_tag = []\n",
    "for l in range(len(word_seq)):\n",
    "    one_list_word += word_seq[l]\n",
    "    one_list_tag += tag_seq[l]\n",
    "\n",
    "one_list_word = ['<BOS>']+one_list_word\n",
    "one_list_word += ['<EOS>']\n",
    "# ong_list_word's content likes '['<BOS>', '迈', '向', '充', '满', '希', '望', ... ,'<EOS>']\n",
    "# one_list_tag's content liked ' ['O', 'O', 'O', 'O', 'O', ...]'\n",
    "# \n",
    "# NOTE: Special attention should be paid that the first and last element in one_list_word, i.e. '<BOS>' and '<EOS>', have no corresponding\n",
    "#       entity tag in one_list_tag\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = segment_by_window(words_list=one_list_word)\n",
    "# X's content likes '[\n",
    "#                     ['<BOS>', '迈', '向'],\n",
    "#                     ['迈', '向', '充'],\n",
    "#                     ['向', '充', '满'],\n",
    "#                     ['充', '满', '希'],\n",
    "#                     ['满', '希', '望'],\n",
    "#                     ['希', '望', '的'],\n",
    "#                      ...\n",
    "#                                      \n",
    "#                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_length = len(X)\n",
    "train_length = int(total_length*0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = one_list_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = X[:train_length], y[:train_length]\n",
    "x_test, y_test = X[train_length:], y[train_length:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train_fitted = extract_feature([x_train])\n",
    "y_train_fitted = [y_train]\n",
    "# x_trian_fitted's content likes '[[\n",
    "#                                  {'bias': 1.0, 'w': '迈', 'w+1': '向', 'w-1': '<BOS>', 'w-1:w': '<BOS>迈', 'w:w+1': '迈向'},\n",
    "#                                  {'bias': 1.0, 'w': '向', 'w+1': '充', 'w-1': '迈', 'w-1:w': '迈向', 'w:w+1': '向充'},\n",
    "#                                  {'bias': 1.0, 'w': '充', 'w+1': '满', 'w-1': '向', 'w-1:w': '向充', 'w:w+1': '充满'},\n",
    "#                                  {'bias': 1.0, 'w': '满', 'w+1': '希', 'w-1': '充', 'w-1:w': '充满', 'w:w+1': '满希'},\n",
    "#                                  {'bias': 1.0, 'w': '希', 'w+1': '望', 'w-1': '满', 'w-1:w': '满希', 'w:w+1': '希望'},\n",
    "#                                   ...\n",
    "#                                 ]]'\n",
    "#\n",
    "# y_train_fitted's content likes '[[\n",
    "#                                   'O',\n",
    "#                                   'O',\n",
    "#                                   'O',\n",
    "#                                   'O',\n",
    "#                                   'O',\n",
    "#                                   ...\n",
    "#                                  ]]'\n",
    "# NOTE: x_train_fitted is a list of list of dicts and y_train_fitted is a list of list of strings. Each element in y_train_fitted[0] \n",
    "#       is the entity tag of the value whose key is 'w' in corresponding dicts, which in x_train_fitted[0]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test_fitted = extract_feature([x_test])\n",
    "y_test_fitted = [y_test]\n",
    "# x_test_fitted and y_test_fitted have the same structure as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练及评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = sklearn_crfsuite.CRF(algorithm='lbfgs',c1=0.1, c2=0.1,max_iterations=100, all_possible_transitions=True)\n",
    "model.fit(x_train_fitted, y_train_fitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 模型的导出与导入\n",
    "\n",
    "joblib.dump(filename=\"CRFmodel_V0.2.model\",value=model)\n",
    "# model = joblib.load(\"CRFmodel_V0.2.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = list(model.classes_)\n",
    "labels.remove('O')\n",
    "y_predict = model.predict(x_test_fitted)\n",
    "metrics.flat_f1_score(y_test_fitted, y_predict, average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(s):\n",
    "    \"\"\"\n",
    "    功能：预测实体标签的函数\n",
    "    输入：一段待检验的话\n",
    "    输出：具有非'O'标签的词语及其实体类别\n",
    "    \"\"\"\n",
    "    # 数据预处理部分 ## code starts here ##\n",
    "    s = q_to_b(s)\n",
    "    s = list(chain(*s))\n",
    "    word_lists = [u'<BOS>']+s+[u'<EOS>']\n",
    "    word_grams = [segment_by_window(word_lists)]\n",
    "    X = extract_feature(word_grams)\n",
    "    # 数据预处理部分 ## code ends here ##\n",
    "    #\n",
    "    # 预测 ## code starts here ##\n",
    "    predict_list = model.predict(X)[0]\n",
    "    # 预测 ## code ends here ##\n",
    "    #\n",
    "    # predict_list 中含有输入的段落中每一个字符（包括标点）的预测实体标签。留下有用实体的内容及标签，去掉'O'及其内容。\n",
    "    #\n",
    "    # 抽取有用内容及标签 ## code starts here\n",
    "    entity_word_list = []\n",
    "    entity_tag_list = []\n",
    "    for i in range(len(predict_list)):\n",
    "        if predict_list[i] != 'O':\n",
    "            entity_word_list.append(s[i])\n",
    "            entity_tag_list.append(predict_list[i])\n",
    "    # 抽取有用内容及标签 ## code ends here\n",
    "    #\n",
    "    entity_word_list.append(\"END\") #加上终止标识，为合并实体做准备\n",
    "    entity_tag_list.append(\"END\")  #加上终止标识，为合并实体标签做准备\n",
    "    #\n",
    "    # 实体及对应标签合并 ## code starts here ##\n",
    "    entity_name = []\n",
    "    entity_tags = []\n",
    "    st = 0\n",
    "    for j in range((len(entity_tag_list)-1)):\n",
    "        if entity_tag_list[j] != entity_tag_list[j+1]:\n",
    "            if r'B' not in entity_tag_list[j]:\n",
    "                entity_name.append(\"\".join(entity_word_list[st:j+1]))\n",
    "                entity_tags.append(\",\".join(entity_tag_list[st:j+1]))\n",
    "                st = j+1\n",
    "    # 实体及对应标签合并 ## code ends here ##\n",
    "    return entity_name, entity_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = '7月11日至12日，公司总经理、党委副书记张凡华和贝多芬、拉格朗日等人先后到广州经研院、上海调控中心等地调研。在经研院，张凡华王龙华听取了经研院工作情况汇报'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entity_name, entity_tags = predict(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_result(entity_name, entity_tags):\n",
    "    for i in range(len(entity_name)):\n",
    "        length = 10 - len(entity_name[i].encode('gbk')) + len(entity_name[i])\n",
    "        s = \"实体内容：%-\"+str(length)+\"s\\t,实体标签：%s\"\n",
    "        print(s % (entity_name[i],entity_tags[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print_result(entity_name, entity_tags)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "451px",
    "left": "879px",
    "top": "107px",
    "width": "273px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
